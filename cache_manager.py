# -*- coding: utf-8 -*-
"""
Cache Manager for FEED-NIDS Pipeline
KB Ïù∏Îç±Ïä§ Î∞è Feedback ÏΩîÌçºÏä§ Ï∫êÏã± ÏãúÏä§ÌÖú
"""
from __future__ import annotations
import os
import json
import hashlib
import pickle
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
import numpy as np
import pandas as pd

try:
    from annoy import AnnoyIndex
    HAS_ANNOY = True
except ImportError:
    HAS_ANNOY = False

from tools.base import get_logger, ensure_dir

log = get_logger("CacheManager")


class CacheManager:
    """
    Ï∫êÏã± ÏãúÏä§ÌÖú Í¥ÄÎ¶¨Ïûê
    - KB Ïù∏Îç±Ïä§ Ï∫êÏã±
    - Feedback ÏΩîÌçºÏä§ Ï∫êÏã±
    - Ï∫êÏãú Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
    """
    
    def __init__(self, cache_root: str = "./cache"):
        self.cache_root = cache_root
        self.kb_cache_dir = os.path.join(cache_root, "kb_index")
        self.fb_cache_dir = os.path.join(cache_root, "feedback_corpus")
        
    # ========================================
    # KB Ïù∏Îç±Ïä§ Ï∫êÏã±
    # ========================================
    
    def get_kb_cache_paths(self) -> Dict[str, str]:
        """KB Ï∫êÏãú ÌååÏùº Í≤ΩÎ°ú Î∞òÌôò"""
        return {
            "scaler": os.path.join(self.kb_cache_dir, "kb_scaler.pkl"),
            "vectors": os.path.join(self.kb_cache_dir, "kb_vectors.npy"),
            "annoy": os.path.join(self.kb_cache_dir, "kb_annoy_index.ann"),
            "metadata": os.path.join(self.kb_cache_dir, "kb_metadata.pkl"),
            "info": os.path.join(self.kb_cache_dir, "kb_cache_info.json"),
        }
    
    def compute_kb_hash(self, kb_corpus: pd.DataFrame) -> str:
        """KB Îç∞Ïù¥ÌÑ∞ Ìï¥Ïãú Í≥ÑÏÇ∞ (Îπ†Î•∏ Í≤ÄÏ¶ùÏö©)"""
        # KB ÌÅ¨Í∏∞ + Ï≤´/ÎßàÏßÄÎßâ 10Í∞ú case_id + label Î∂ÑÌè¨Î°ú Ìï¥Ïãú
        hash_input = f"{len(kb_corpus)}"
        
        if "case_id" in kb_corpus.columns:
            first_ids = kb_corpus["case_id"].head(10).astype(str).tolist()
            last_ids = kb_corpus["case_id"].tail(10).astype(str).tolist()
            hash_input += "_" + "_".join(first_ids + last_ids)
        
        if "label" in kb_corpus.columns:
            label_dist = kb_corpus["label"].value_counts().to_dict()
            hash_input += "_" + str(sorted(label_dist.items()))
        
        return hashlib.md5(hash_input.encode()).hexdigest()
    
    def is_kb_cache_valid(
        self,
        kb_corpus: pd.DataFrame,
        common_features: list,
        alpha: float,
        beta: float,
        gamma: float,
        n_trees: int,
    ) -> bool:
        """KB Ï∫êÏãú Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù"""
        paths = self.get_kb_cache_paths()
        
        # 1. ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä
        if not all(os.path.exists(p) for p in paths.values()):
            log.info("‚ùå Ï∫êÏãú ÌååÏùº ÏóÜÏùå")
            return False
        
        # 2. Ï∫êÏãú Ï†ïÎ≥¥ Î°úÎìú
        try:
            with open(paths["info"], "r", encoding="utf-8") as f:
                cache_info = json.load(f)
        except Exception as e:
            log.warning(f"‚ùå Ï∫êÏãú Ï†ïÎ≥¥ Î°úÎìú Ïã§Ìå®: {e}")
            return False
        
        # 3. KB ÌÅ¨Í∏∞ ÏùºÏπò
        if cache_info.get("kb_size") != len(kb_corpus):
            log.info(f"‚ùå KB ÌÅ¨Í∏∞ Î∂àÏùºÏπò: Ï∫êÏãú={cache_info.get('kb_size')}, ÌòÑÏû¨={len(kb_corpus)}")
            return False
        
        # 4. ÌîºÏ≤ò Ïàò ÏùºÏπò
        if cache_info.get("n_features") != len(common_features):
            log.info(f"‚ùå ÌîºÏ≤ò Ïàò Î∂àÏùºÏπò: Ï∫êÏãú={cache_info.get('n_features')}, ÌòÑÏû¨={len(common_features)}")
            return False
        
        # 5. ÌååÎùºÎØ∏ÌÑ∞ ÏùºÏπò
        if (cache_info.get("alpha") != alpha or
            cache_info.get("beta") != beta or
            cache_info.get("gamma") != gamma or
            cache_info.get("n_trees") != n_trees):
            log.info("‚ùå ÌååÎùºÎØ∏ÌÑ∞ Î∂àÏùºÏπò")
            return False
        
        # 6. Îç∞Ïù¥ÌÑ∞ Ìï¥Ïãú ÏùºÏπò (ÏÑ†ÌÉùÏ†Å)
        current_hash = self.compute_kb_hash(kb_corpus)
        if cache_info.get("kb_hash") != current_hash:
            log.info("‚ùå KB Îç∞Ïù¥ÌÑ∞ Ìï¥Ïãú Î∂àÏùºÏπò (ÎÇ¥Ïö© Î≥ÄÍ≤ΩÎê®)")
            return False
        
        log.info("‚úÖ KB Ï∫êÏãú Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù ÌÜµÍ≥º")
        return True
    
    def load_kb_cache(
        self,
        n_features: int,
    ) -> Tuple[object, np.ndarray, AnnoyIndex, Dict[str, Any]]:
        """
        KB Ï∫êÏãú Î°úÎìú
        Returns: (scaler, kb_vectors_normalized, annoy_index, metadata)
        """
        log.info("üìÇ KB Ï∫êÏãú Î°úÎìú Ï§ë...")
        paths = self.get_kb_cache_paths()
        
        # 1. Scaler Î°úÎìú
        with open(paths["scaler"], "rb") as f:
            scaler = pickle.load(f)
        log.info("  ‚úÖ Scaler Î°úÎìú ÏôÑÎ£å")
        
        # 2. Î≤°ÌÑ∞ Î°úÎìú
        kb_vectors_normalized = np.load(paths["vectors"])
        log.info(f"  ‚úÖ KB Î≤°ÌÑ∞ Î°úÎìú ÏôÑÎ£å: {kb_vectors_normalized.shape}")
        
        # 3. Annoy Ïù∏Îç±Ïä§ Î°úÎìú
        if not HAS_ANNOY:
            raise ImportError("AnnoyÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§: pip install annoy")
        
        annoy_index = AnnoyIndex(n_features, 'angular')
        annoy_index.load(paths["annoy"])
        log.info(f"  ‚úÖ Annoy Ïù∏Îç±Ïä§ Î°úÎìú ÏôÑÎ£å: {annoy_index.get_n_items()} items")
        
        # 4. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú
        with open(paths["metadata"], "rb") as f:
            metadata = pickle.load(f)
        log.info("  ‚úÖ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å")
        
        log.info("‚úÖ KB Ï∫êÏãú Î°úÎìú ÏôÑÎ£å (ÏïΩ 5-10Ï¥à)")
        return scaler, kb_vectors_normalized, annoy_index, metadata
    
    def save_kb_cache(
        self,
        scaler: object,
        kb_vectors_normalized: np.ndarray,
        annoy_index: AnnoyIndex,
        metadata: Dict[str, Any],
        kb_corpus: pd.DataFrame,
        common_features: list,
        alpha: float,
        beta: float,
        gamma: float,
        n_trees: int,
    ) -> None:
        """KB Ï∫êÏãú Ï†ÄÏû•"""
        log.info("üíæ KB Ï∫êÏãú Ï†ÄÏû• Ï§ë...")
        ensure_dir(self.kb_cache_dir)
        paths = self.get_kb_cache_paths()
        
        # 1. Scaler Ï†ÄÏû•
        with open(paths["scaler"], "wb") as f:
            pickle.dump(scaler, f)
        log.info("  ‚úÖ Scaler Ï†ÄÏû• ÏôÑÎ£å")
        
        # 2. Î≤°ÌÑ∞ Ï†ÄÏû•
        np.save(paths["vectors"], kb_vectors_normalized)
        log.info(f"  ‚úÖ KB Î≤°ÌÑ∞ Ï†ÄÏû• ÏôÑÎ£å: {kb_vectors_normalized.shape}")
        
        # 3. Annoy Ïù∏Îç±Ïä§ Ï†ÄÏû•
        annoy_index.save(paths["annoy"])
        log.info(f"  ‚úÖ Annoy Ïù∏Îç±Ïä§ Ï†ÄÏû• ÏôÑÎ£å")
        
        # 4. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
        with open(paths["metadata"], "wb") as f:
            pickle.dump(metadata, f)
        log.info("  ‚úÖ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû• ÏôÑÎ£å")
        
        # 5. Ï∫êÏãú Ï†ïÎ≥¥ Ï†ÄÏû•
        cache_info = {
            "kb_size": len(kb_corpus),
            "n_features": len(common_features),
            "kb_hash": self.compute_kb_hash(kb_corpus),
            "created_at": datetime.now().isoformat(),
            "alpha": alpha,
            "beta": beta,
            "gamma": gamma,
            "n_trees": n_trees,
            "cache_version": "1.0",
        }
        with open(paths["info"], "w", encoding="utf-8") as f:
            json.dump(cache_info, f, indent=2, ensure_ascii=False)
        log.info("  ‚úÖ Ï∫êÏãú Ï†ïÎ≥¥ Ï†ÄÏû• ÏôÑÎ£å")
        
        log.info("‚úÖ KB Ï∫êÏãú Ï†ÄÏû• ÏôÑÎ£å")
    
    def clear_kb_cache(self) -> None:
        """KB Ï∫êÏãú ÏÇ≠Ï†ú"""
        paths = self.get_kb_cache_paths()
        deleted = 0
        for name, path in paths.items():
            if os.path.exists(path):
                try:
                    os.remove(path)
                    deleted += 1
                    log.info(f"  ‚úÖ ÏÇ≠Ï†ú: {name}")
                except Exception as e:
                    log.warning(f"  ‚ö†Ô∏è ÏÇ≠Ï†ú Ïã§Ìå®: {name} ‚Üí {e}")
        
        if deleted > 0:
            log.info(f"‚úÖ KB Ï∫êÏãú ÏÇ≠Ï†ú ÏôÑÎ£å: {deleted}Í∞ú ÌååÏùº")
        else:
            log.info("‚ÑπÔ∏è ÏÇ≠Ï†úÌï† KB Ï∫êÏãú ÏóÜÏùå")
    
    # ========================================
    # Feedback ÏΩîÌçºÏä§ Ï∫êÏã±
    # ========================================
    
    def get_fb_cache_paths(self, round_name: str) -> Dict[str, str]:
        """Feedback ÏΩîÌçºÏä§ Ï∫êÏãú ÌååÏùº Í≤ΩÎ°ú Î∞òÌôò"""
        return {
            "scaler": os.path.join(self.fb_cache_dir, f"{round_name}_fb_scaler.pkl"),
            "vectors": os.path.join(self.fb_cache_dir, f"{round_name}_fb_vectors.npy"),
            "annoy": os.path.join(self.fb_cache_dir, f"{round_name}_fb_annoy_index.ann"),
            "metadata": os.path.join(self.fb_cache_dir, f"{round_name}_fb_metadata.pkl"),
            "info": os.path.join(self.fb_cache_dir, f"{round_name}_fb_cache_info.json"),
        }
    
    def compute_fb_hash(self, feedback_corpus: pd.DataFrame) -> str:
        """Feedback ÏΩîÌçºÏä§ Ìï¥Ïãú Í≥ÑÏÇ∞"""
        hash_input = f"{len(feedback_corpus)}"
        
        if "case_id" in feedback_corpus.columns:
            case_ids = feedback_corpus["case_id"].astype(str).tolist()
            hash_input += "_" + "_".join(sorted(case_ids[:20]))  # Ï≤´ 20Í∞úÎßå
        
        if "feedback_label" in feedback_corpus.columns:
            label_dist = feedback_corpus["feedback_label"].value_counts().to_dict()
            hash_input += "_" + str(sorted(label_dist.items()))
        
        return hashlib.md5(hash_input.encode()).hexdigest()
    
    def is_fb_cache_valid(
        self,
        round_name: str,
        feedback_corpus: pd.DataFrame,
        common_features: list,
        alpha: float,
        beta: float,
        gamma: float,
        n_trees: int,
    ) -> bool:
        """Feedback Ï∫êÏãú Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù"""
        paths = self.get_fb_cache_paths(round_name)
        
        # 1. ÌååÏùº Ï°¥Ïû¨ Ïó¨Î∂Ä
        if not all(os.path.exists(p) for p in paths.values()):
            log.info(f"‚ùå [{round_name}] Feedback Ï∫êÏãú ÌååÏùº ÏóÜÏùå")
            return False
        
        # 2. Ï∫êÏãú Ï†ïÎ≥¥ Î°úÎìú
        try:
            with open(paths["info"], "r", encoding="utf-8") as f:
                cache_info = json.load(f)
        except Exception as e:
            log.warning(f"‚ùå [{round_name}] Feedback Ï∫êÏãú Ï†ïÎ≥¥ Î°úÎìú Ïã§Ìå®: {e}")
            return False
        
        # 3. ÏΩîÌçºÏä§ ÌÅ¨Í∏∞ ÏùºÏπò
        if cache_info.get("corpus_size") != len(feedback_corpus):
            log.info(f"‚ùå [{round_name}] Feedback ÏΩîÌçºÏä§ ÌÅ¨Í∏∞ Î∂àÏùºÏπò")
            return False
        
        # 4. ÌîºÏ≤ò Ïàò ÏùºÏπò
        if cache_info.get("n_features") != len(common_features):
            log.info(f"‚ùå [{round_name}] ÌîºÏ≤ò Ïàò Î∂àÏùºÏπò")
            return False
        
        # 5. ÌååÎùºÎØ∏ÌÑ∞ ÏùºÏπò
        if (cache_info.get("alpha") != alpha or
            cache_info.get("beta") != beta or
            cache_info.get("gamma") != gamma or
            cache_info.get("n_trees") != n_trees):
            log.info(f"‚ùå [{round_name}] ÌååÎùºÎØ∏ÌÑ∞ Î∂àÏùºÏπò")
            return False
        
        # 6. Îç∞Ïù¥ÌÑ∞ Ìï¥Ïãú ÏùºÏπò
        current_hash = self.compute_fb_hash(feedback_corpus)
        if cache_info.get("corpus_hash") != current_hash:
            log.info(f"‚ùå [{round_name}] Feedback ÏΩîÌçºÏä§ Ìï¥Ïãú Î∂àÏùºÏπò")
            return False
        
        log.info(f"‚úÖ [{round_name}] Feedback Ï∫êÏãú Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù ÌÜµÍ≥º")
        return True
    
    def load_fb_cache(
        self,
        round_name: str,
        n_features: int,
    ) -> Tuple[object, np.ndarray, AnnoyIndex, Dict[str, Any]]:
        """
        Feedback Ï∫êÏãú Î°úÎìú
        Returns: (scaler, fb_vectors_normalized, annoy_index, metadata)
        """
        log.info(f"üìÇ [{round_name}] Feedback Ï∫êÏãú Î°úÎìú Ï§ë...")
        paths = self.get_fb_cache_paths(round_name)
        
        # 1. Scaler Î°úÎìú
        with open(paths["scaler"], "rb") as f:
            scaler = pickle.load(f)
        
        # 2. Î≤°ÌÑ∞ Î°úÎìú
        fb_vectors_normalized = np.load(paths["vectors"])
        
        # 3. Annoy Ïù∏Îç±Ïä§ Î°úÎìú
        if not HAS_ANNOY:
            raise ImportError("AnnoyÍ∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§: pip install annoy")
        
        annoy_index = AnnoyIndex(n_features, 'angular')
        annoy_index.load(paths["annoy"])
        
        # 4. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú
        with open(paths["metadata"], "rb") as f:
            metadata = pickle.load(f)
        
        log.info(f"‚úÖ [{round_name}] Feedback Ï∫êÏãú Î°úÎìú ÏôÑÎ£å")
        return scaler, fb_vectors_normalized, annoy_index, metadata
    
    def save_fb_cache(
        self,
        round_name: str,
        scaler: object,
        fb_vectors_normalized: np.ndarray,
        annoy_index: AnnoyIndex,
        metadata: Dict[str, Any],
        feedback_corpus: pd.DataFrame,
        common_features: list,
        alpha: float,
        beta: float,
        gamma: float,
        n_trees: int,
    ) -> None:
        """Feedback Ï∫êÏãú Ï†ÄÏû•"""
        log.info(f"üíæ [{round_name}] Feedback Ï∫êÏãú Ï†ÄÏû• Ï§ë...")
        ensure_dir(self.fb_cache_dir)
        paths = self.get_fb_cache_paths(round_name)
        
        # 1. Scaler Ï†ÄÏû•
        with open(paths["scaler"], "wb") as f:
            pickle.dump(scaler, f)
        
        # 2. Î≤°ÌÑ∞ Ï†ÄÏû•
        np.save(paths["vectors"], fb_vectors_normalized)
        
        # 3. Annoy Ïù∏Îç±Ïä§ Ï†ÄÏû•
        annoy_index.save(paths["annoy"])
        
        # 4. Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
        with open(paths["metadata"], "wb") as f:
            pickle.dump(metadata, f)
        
        # 5. Ï∫êÏãú Ï†ïÎ≥¥ Ï†ÄÏû•
        cache_info = {
            "round_name": round_name,
            "corpus_size": len(feedback_corpus),
            "n_features": len(common_features),
            "corpus_hash": self.compute_fb_hash(feedback_corpus),
            "created_at": datetime.now().isoformat(),
            "alpha": alpha,
            "beta": beta,
            "gamma": gamma,
            "n_trees": n_trees,
            "cache_version": "1.0",
        }
        with open(paths["info"], "w", encoding="utf-8") as f:
            json.dump(cache_info, f, indent=2, ensure_ascii=False)
        
        log.info(f"‚úÖ [{round_name}] Feedback Ï∫êÏãú Ï†ÄÏû• ÏôÑÎ£å")
    
    def clear_fb_cache(self, round_name: Optional[str] = None) -> None:
        """Feedback Ï∫êÏãú ÏÇ≠Ï†ú (round_name ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥)"""
        if round_name:
            # ÌäπÏ†ï ÎùºÏö¥ÎìúÎßå ÏÇ≠Ï†ú
            paths = self.get_fb_cache_paths(round_name)
            deleted = 0
            for name, path in paths.items():
                if os.path.exists(path):
                    try:
                        os.remove(path)
                        deleted += 1
                    except Exception as e:
                        log.warning(f"  ‚ö†Ô∏è ÏÇ≠Ï†ú Ïã§Ìå®: {name} ‚Üí {e}")
            
            if deleted > 0:
                log.info(f"‚úÖ [{round_name}] Feedback Ï∫êÏãú ÏÇ≠Ï†ú ÏôÑÎ£å: {deleted}Í∞ú ÌååÏùº")
            else:
                log.info(f"‚ÑπÔ∏è [{round_name}] ÏÇ≠Ï†úÌï† Feedback Ï∫êÏãú ÏóÜÏùå")
        else:
            # Ï†ÑÏ≤¥ ÏÇ≠Ï†ú
            if os.path.exists(self.fb_cache_dir):
                import shutil
                try:
                    shutil.rmtree(self.fb_cache_dir)
                    log.info("‚úÖ Ï†ÑÏ≤¥ Feedback Ï∫êÏãú ÏÇ≠Ï†ú ÏôÑÎ£å")
                except Exception as e:
                    log.warning(f"‚ö†Ô∏è Feedback Ï∫êÏãú ÏÇ≠Ï†ú Ïã§Ìå®: {e}")
            else:
                log.info("‚ÑπÔ∏è ÏÇ≠Ï†úÌï† Feedback Ï∫êÏãú ÏóÜÏùå")
    
    def clear_all_cache(self) -> None:
        """Î™®Îì† Ï∫êÏãú ÏÇ≠Ï†ú"""
        log.info("üóëÔ∏è Ï†ÑÏ≤¥ Ï∫êÏãú ÏÇ≠Ï†ú Ï§ë...")
        self.clear_kb_cache()
        self.clear_fb_cache()
        log.info("‚úÖ Ï†ÑÏ≤¥ Ï∫êÏãú ÏÇ≠Ï†ú ÏôÑÎ£å")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """Ï∫êÏãú ÏÉÅÌÉú Ï†ïÎ≥¥ Î∞òÌôò"""
        info = {
            "kb_cache": {},
            "fb_cache": {},
        }
        
        # KB Ï∫êÏãú Ï†ïÎ≥¥
        kb_info_path = self.get_kb_cache_paths()["info"]
        if os.path.exists(kb_info_path):
            try:
                with open(kb_info_path, "r", encoding="utf-8") as f:
                    info["kb_cache"] = json.load(f)
            except Exception:
                pass
        
        # Feedback Ï∫êÏãú Ï†ïÎ≥¥
        if os.path.exists(self.fb_cache_dir):
            fb_files = [f for f in os.listdir(self.fb_cache_dir) if f.endswith("_fb_cache_info.json")]
            for fb_file in fb_files:
                try:
                    with open(os.path.join(self.fb_cache_dir, fb_file), "r", encoding="utf-8") as f:
                        fb_info = json.load(f)
                        round_name = fb_info.get("round_name", "unknown")
                        info["fb_cache"][round_name] = fb_info
                except Exception:
                    pass
        
        return info